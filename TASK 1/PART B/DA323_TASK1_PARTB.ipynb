{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f49f312",
   "metadata": {},
   "source": [
    "\n",
    "# **TASK B** : Text Dataset Collection: Web Crawler and NLP Preprocessing\n",
    "  \n",
    "\n",
    " \n",
    "- **This notebook demonstrates:**\n",
    "    - 1. Listing 20 different categories and selecting three websites for each.\n",
    "    - 2. Implementing a web crawler to extract relevant text (article title, content, publication date) from these websites.\n",
    "    - 3. Storing the collected data into 20 text files (one per category).\n",
    "    - 4. Cleaning the text using NLP preprocessing (removing HTML tags, punctuation, and stop words).\n",
    "    - 5. Naming the dataset and demonstrating a use case (here, a simple frequency analysis).\n",
    "\n",
    "## **Dataset Name:** `IITG_MultimodalTextDataset`\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab966f6",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "- We will import necessary libraries including `requests`, `BeautifulSoup` for web scraping, `nltk` for NLP preprocessing, and other utilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04f24939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# For NLP preprocessing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0191aaef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\iitia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\iitia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download nltk stopwords if not already available\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93368ace",
   "metadata": {},
   "source": [
    "## 2. Define Categories and Websites\n",
    "\n",
    "- We define 20 categories and for each, list three example websites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b43892b",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {\n",
    "    \"Technology\": [\n",
    "        \"https://techcrunch.com\",\n",
    "        \"https://www.wired.com\",\n",
    "        \"https://www.theverge.com\"\n",
    "    ],\n",
    "    \"Sports\": [\n",
    "        \"https://www.espn.com\",\n",
    "        \"https://www.bbc.com/sport\",\n",
    "        \"https://www.si.com\"\n",
    "    ],\n",
    "    \"Health\": [\n",
    "        \"https://www.webmd.com\",\n",
    "        \"https://www.healthline.com\",\n",
    "        \"https://www.medicalnewstoday.com\"\n",
    "    ],\n",
    "    \"Politics\": [\n",
    "        \"https://www.cnn.com/politics\",\n",
    "        \"https://www.bbc.com/news/politics\",\n",
    "        \"https://www.nbcnews.com/politics\"\n",
    "    ],\n",
    "    \"Entertainment\": [\n",
    "        \"https://ew.com\",\n",
    "        \"https://www.rollingstone.com\",\n",
    "        \"https://variety.com\"\n",
    "    ],\n",
    "    \"Business\": [\n",
    "        \"https://www.businessinsider.com\",\n",
    "        \"https://www.forbes.com\",\n",
    "        \"https://www.cnbc.com\"\n",
    "    ],\n",
    "    \"Science\": [\n",
    "        \"https://www.livescience.com\",\n",
    "        \"https://www.nature.com\",\n",
    "        \"https://www.scientificamerican.com\"\n",
    "    ],\n",
    "    \"Education\": [\n",
    "        \"https://www.edutopia.org\",\n",
    "        \"https://www.insidehighered.com\",\n",
    "        \"https://www.timeshighereducation.com\"\n",
    "    ],\n",
    "    \"Environment\": [\n",
    "        \"https://www.nationalgeographic.com/environment\",\n",
    "        \"https://www.theguardian.com/environment\",\n",
    "        \"https://www.ecowatch.com\"\n",
    "    ],\n",
    "    \"Travel\": [\n",
    "        \"https://www.lonelyplanet.com\",\n",
    "        \"https://www.cntraveler.com\",\n",
    "        \"https://www.travelandleisure.com\"\n",
    "    ],\n",
    "    \"Food\": [\n",
    "        \"https://www.foodnetwork.com\",\n",
    "        \"https://www.seriouseats.com\",\n",
    "        \"https://www.bonappetit.com\"\n",
    "    ],\n",
    "    \"Fashion\": [\n",
    "        \"https://www.vogue.com\",\n",
    "        \"https://www.elle.com\",\n",
    "        \"https://www.harpersbazaar.com\"\n",
    "    ],\n",
    "    \"Art & Culture\": [\n",
    "        \"https://www.artsy.net\",\n",
    "        \"https://www.theartnewspaper.com\",\n",
    "        \"https://www.artforum.com\"\n",
    "    ],\n",
    "    \"Finance\": [\n",
    "        \"https://www.ft.com\",\n",
    "        \"https://www.investopedia.com\",\n",
    "        \"https://www.fool.com\"\n",
    "    ],\n",
    "    \"History\": [\n",
    "        \"https://www.history.com\",\n",
    "        \"https://www.bbc.co.uk/history\",\n",
    "        \"https://www.historyextra.com\"\n",
    "    ],\n",
    "    \"Literature\": [\n",
    "        \"https://lithub.com\",\n",
    "        \"https://themillions.com\",\n",
    "        \"https://www.poetryfoundation.org\"\n",
    "    ],\n",
    "    \"Music\": [\n",
    "        \"https://pitchfork.com\",\n",
    "        \"https://www.rollingstone.com/music\",\n",
    "        \"https://www.nme.com\"\n",
    "    ],\n",
    "    \"Gaming\": [\n",
    "        \"https://www.polygon.com\",\n",
    "        \"https://www.gamesradar.com\",\n",
    "        \"https://www.destructoid.com\"\n",
    "    ],\n",
    "    \"Lifestyle\": [\n",
    "        \"https://www.lifehacker.com\",\n",
    "        \"https://www.thecut.com\",\n",
    "        \"https://www.refinery29.com\"\n",
    "    ],\n",
    "    \"World News\": [\n",
    "        \"https://www.bbc.com/news/world\",\n",
    "        \"https://www.aljazeera.com\",\n",
    "        \"https://www.cnn.com/world\"\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887669ab",
   "metadata": {},
   "source": [
    "## 3. Define Functions for Web Crawling and Text Extraction\n",
    "\n",
    "- **We define a function `crawl_website` which:**\n",
    "     - Sends a GET request to the website.\n",
    "     - Uses BeautifulSoup to parse the HTML.\n",
    "     - Extracts the page title.\n",
    "     - Tries to find an `<article>` tag and extracts paragraphs from it (if available).\n",
    "     - Extracts a simulated publication date (if available in meta tags).\n",
    "    ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a41601bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_website(url):\n",
    "    \"\"\"\n",
    "    Crawl the given URL and extract article title, content, and publication date.\n",
    "    Returns a dictionary with keys: 'title', 'content', 'date'.\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (compatible; IITGDataCrawler/1.0)\"\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Extract title (from <title> or first <h1>)\n",
    "    title = soup.title.string if soup.title else \"No Title\"\n",
    "    \n",
    "    # Attempt to extract article content: look for <article> tag or all <p> tags\n",
    "    article = soup.find('article')\n",
    "    if article:\n",
    "        paragraphs = article.find_all('p')\n",
    "    else:\n",
    "        paragraphs = soup.find_all('p')\n",
    "    \n",
    "    content = \"\\n\".join([p.get_text() for p in paragraphs])\n",
    "    \n",
    "    # Extract publication date from meta tags (this is a simple heuristic)\n",
    "    pub_date = None\n",
    "    for meta in soup.find_all('meta'):\n",
    "        if meta.get(\"name\", \"\").lower() in [\"pubdate\", \"publication_date\", \"date\"]:\n",
    "            pub_date = meta.get(\"content\")\n",
    "            break\n",
    "    \n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"content\": content,\n",
    "        \"date\": pub_date if pub_date else \"Unknown\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6a6735",
   "metadata": {},
   "source": [
    "## 4. Define Function for Cleaning Text\n",
    "- **This function cleans the text by:**\n",
    "     - Removing HTML tags (if any remain).\n",
    "     - Lowercasing the text.\n",
    "     - Removing punctuation.\n",
    "     - Removing stop words using NLTK.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "83184275",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove any residual HTML tags\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    \n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Define a custom punctuation string including typical punctuation and additional typographic quotes/dashes\n",
    "    custom_punct = string.punctuation + \"“”‘’—–\"\n",
    "    \n",
    "    # Remove punctuation using regex (all characters in custom_punct)\n",
    "    text = re.sub(f\"[{re.escape(custom_punct)}]\", \"\", text)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    cleaned_tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Rejoin tokens into a cleaned string\n",
    "    cleaned_text = \" \".join(cleaned_tokens)\n",
    "    return cleaned_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e89152",
   "metadata": {},
   "source": [
    "## 5. Crawl and Store Data for Each Category\n",
    "- For each category, we iterate through its three websites, crawl the pages, extract the text, and clean it.\n",
    "- The results are then stored in a text file named after the category.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd7ded64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing category: Technology\n",
      "  Crawling https://techcrunch.com\n",
      "  Crawling https://www.wired.com\n",
      "  Crawling https://www.theverge.com\n",
      "  Raw data for category 'Technology' saved to text_dataset\\raw\\Technology.txt\n",
      "  Cleaned data for category 'Technology' saved to text_dataset\\cleaned\\Technology.txt\n",
      "Processing category: Sports\n",
      "  Crawling https://www.espn.com\n",
      "  Crawling https://www.bbc.com/sport\n",
      "  Crawling https://www.si.com\n",
      "  Raw data for category 'Sports' saved to text_dataset\\raw\\Sports.txt\n",
      "  Cleaned data for category 'Sports' saved to text_dataset\\cleaned\\Sports.txt\n",
      "Processing category: Health\n",
      "  Crawling https://www.webmd.com\n",
      "  Crawling https://www.healthline.com\n",
      "  Crawling https://www.medicalnewstoday.com\n",
      "  Raw data for category 'Health' saved to text_dataset\\raw\\Health.txt\n",
      "  Cleaned data for category 'Health' saved to text_dataset\\cleaned\\Health.txt\n",
      "Processing category: Politics\n",
      "  Crawling https://www.cnn.com/politics\n",
      "  Crawling https://www.bbc.com/news/politics\n",
      "  Crawling https://www.nbcnews.com/politics\n",
      "  Raw data for category 'Politics' saved to text_dataset\\raw\\Politics.txt\n",
      "  Cleaned data for category 'Politics' saved to text_dataset\\cleaned\\Politics.txt\n",
      "Processing category: Entertainment\n",
      "  Crawling https://ew.com\n",
      "  Crawling https://www.rollingstone.com\n",
      "  Crawling https://variety.com\n",
      "  Raw data for category 'Entertainment' saved to text_dataset\\raw\\Entertainment.txt\n",
      "  Cleaned data for category 'Entertainment' saved to text_dataset\\cleaned\\Entertainment.txt\n",
      "Processing category: Business\n",
      "  Crawling https://www.businessinsider.com\n",
      "  Crawling https://www.forbes.com\n",
      "  Crawling https://www.cnbc.com\n",
      "  Raw data for category 'Business' saved to text_dataset\\raw\\Business.txt\n",
      "  Cleaned data for category 'Business' saved to text_dataset\\cleaned\\Business.txt\n",
      "Processing category: Science\n",
      "  Crawling https://www.livescience.com\n",
      "  Crawling https://www.nature.com\n",
      "  Crawling https://www.scientificamerican.com\n",
      "  Raw data for category 'Science' saved to text_dataset\\raw\\Science.txt\n",
      "  Cleaned data for category 'Science' saved to text_dataset\\cleaned\\Science.txt\n",
      "Processing category: Education\n",
      "  Crawling https://www.edutopia.org\n",
      "  Crawling https://www.insidehighered.com\n",
      "  Crawling https://www.timeshighereducation.com\n",
      "  Raw data for category 'Education' saved to text_dataset\\raw\\Education.txt\n",
      "  Cleaned data for category 'Education' saved to text_dataset\\cleaned\\Education.txt\n",
      "Processing category: Environment\n",
      "  Crawling https://www.nationalgeographic.com/environment\n",
      "  Crawling https://www.theguardian.com/environment\n",
      "  Crawling https://www.ecowatch.com\n",
      "  Raw data for category 'Environment' saved to text_dataset\\raw\\Environment.txt\n",
      "  Cleaned data for category 'Environment' saved to text_dataset\\cleaned\\Environment.txt\n",
      "Processing category: Travel\n",
      "  Crawling https://www.lonelyplanet.com\n",
      "  Crawling https://www.cntraveler.com\n",
      "  Crawling https://www.travelandleisure.com\n",
      "  Raw data for category 'Travel' saved to text_dataset\\raw\\Travel.txt\n",
      "  Cleaned data for category 'Travel' saved to text_dataset\\cleaned\\Travel.txt\n",
      "Processing category: Food\n",
      "  Crawling https://www.foodnetwork.com\n",
      "  Crawling https://www.seriouseats.com\n",
      "  Crawling https://www.bonappetit.com\n",
      "  Raw data for category 'Food' saved to text_dataset\\raw\\Food.txt\n",
      "  Cleaned data for category 'Food' saved to text_dataset\\cleaned\\Food.txt\n",
      "Processing category: Fashion\n",
      "  Crawling https://www.vogue.com\n",
      "  Crawling https://www.elle.com\n",
      "  Crawling https://www.harpersbazaar.com\n",
      "  Raw data for category 'Fashion' saved to text_dataset\\raw\\Fashion.txt\n",
      "  Cleaned data for category 'Fashion' saved to text_dataset\\cleaned\\Fashion.txt\n",
      "Processing category: Art & Culture\n",
      "  Crawling https://www.artsy.net\n",
      "  Crawling https://www.theartnewspaper.com\n",
      "  Crawling https://www.artforum.com\n",
      "  Raw data for category 'Art & Culture' saved to text_dataset\\raw\\Art_&_Culture.txt\n",
      "  Cleaned data for category 'Art & Culture' saved to text_dataset\\cleaned\\Art_&_Culture.txt\n",
      "Processing category: Finance\n",
      "  Crawling https://www.ft.com\n",
      "  Crawling https://www.investopedia.com\n",
      "  Crawling https://www.fool.com\n",
      "  Raw data for category 'Finance' saved to text_dataset\\raw\\Finance.txt\n",
      "  Cleaned data for category 'Finance' saved to text_dataset\\cleaned\\Finance.txt\n",
      "Processing category: History\n",
      "  Crawling https://www.history.com\n",
      "  Crawling https://www.bbc.co.uk/history\n",
      "  Crawling https://www.historyextra.com\n",
      "  Raw data for category 'History' saved to text_dataset\\raw\\History.txt\n",
      "  Cleaned data for category 'History' saved to text_dataset\\cleaned\\History.txt\n",
      "Processing category: Literature\n",
      "  Crawling https://lithub.com\n",
      "  Crawling https://themillions.com\n",
      "  Crawling https://www.poetryfoundation.org\n",
      "  Raw data for category 'Literature' saved to text_dataset\\raw\\Literature.txt\n",
      "  Cleaned data for category 'Literature' saved to text_dataset\\cleaned\\Literature.txt\n",
      "Processing category: Music\n",
      "  Crawling https://pitchfork.com\n",
      "  Crawling https://www.rollingstone.com/music\n",
      "  Crawling https://www.nme.com\n",
      "  Raw data for category 'Music' saved to text_dataset\\raw\\Music.txt\n",
      "  Cleaned data for category 'Music' saved to text_dataset\\cleaned\\Music.txt\n",
      "Processing category: Gaming\n",
      "  Crawling https://www.polygon.com\n",
      "  Crawling https://www.gamesradar.com\n",
      "  Crawling https://www.destructoid.com\n",
      "Error fetching https://www.destructoid.com: HTTPSConnectionPool(host='www.destructoid.com', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1006)')))\n",
      "  Raw data for category 'Gaming' saved to text_dataset\\raw\\Gaming.txt\n",
      "  Cleaned data for category 'Gaming' saved to text_dataset\\cleaned\\Gaming.txt\n",
      "Processing category: Lifestyle\n",
      "  Crawling https://www.lifehacker.com\n",
      "  Crawling https://www.thecut.com\n",
      "  Crawling https://www.refinery29.com\n",
      "  Raw data for category 'Lifestyle' saved to text_dataset\\raw\\Lifestyle.txt\n",
      "  Cleaned data for category 'Lifestyle' saved to text_dataset\\cleaned\\Lifestyle.txt\n",
      "Processing category: World News\n",
      "  Crawling https://www.bbc.com/news/world\n",
      "  Crawling https://www.aljazeera.com\n",
      "  Crawling https://www.cnn.com/world\n",
      "  Raw data for category 'World News' saved to text_dataset\\raw\\World_News.txt\n",
      "  Cleaned data for category 'World News' saved to text_dataset\\cleaned\\World_News.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a main directory for the dataset and two subdirectories for raw and cleaned data\n",
    "output_dir = \"IITG_MultimodalTextDataset\"\n",
    "raw_dir = os.path.join(output_dir, \"raw\")\n",
    "cleaned_dir = os.path.join(output_dir, \"cleaned\")\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "if not os.path.exists(raw_dir):\n",
    "    os.makedirs(raw_dir)\n",
    "if not os.path.exists(cleaned_dir):\n",
    "    os.makedirs(cleaned_dir)\n",
    "\n",
    "# Iterate over each category and its websites\n",
    "for category, urls in categories.items():\n",
    "    print(f\"Processing category: {category}\")\n",
    "    \n",
    "    # Lists to store raw and cleaned text entries\n",
    "    raw_texts = []\n",
    "    cleaned_texts = []\n",
    "    \n",
    "    for url in urls:\n",
    "        print(f\"  Crawling {url}\")\n",
    "        data = crawl_website(url)\n",
    "        if data:\n",
    "            # Create the raw entry\n",
    "            entry_raw = (f\"Title: {data['title']}\\n\"\n",
    "                         f\"Publication Date: {data['date']}\\n\"\n",
    "                         f\"Content:\\n{data['content']}\\n\"\n",
    "                         f\"{'-'*80}\\n\")\n",
    "            raw_texts.append(entry_raw)\n",
    "            \n",
    "            # Clean the raw entry and add to cleaned list\n",
    "            entry_cleaned = clean_text(entry_raw)\n",
    "            cleaned_texts.append(entry_cleaned)\n",
    "    \n",
    "    # Combine all entries for the category\n",
    "    combined_raw = \"\\n\".join(raw_texts)\n",
    "    combined_cleaned = \"\\n\".join(cleaned_texts)\n",
    "    \n",
    "    # Save raw data file in the raw folder\n",
    "    file_path_raw = os.path.join(raw_dir, f\"{category.replace(' ', '_')}.txt\")\n",
    "    with open(file_path_raw, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(combined_raw)\n",
    "    \n",
    "    # Save cleaned data file in the cleaned folder\n",
    "    file_path_cleaned = os.path.join(cleaned_dir, f\"{category.replace(' ', '_')}.txt\")\n",
    "    with open(file_path_cleaned, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(combined_cleaned)\n",
    "    \n",
    "    print(f\"  Raw data for category '{category}' saved to {file_path_raw}\")\n",
    "    print(f\"  Cleaned data for category '{category}' saved to {file_path_cleaned}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d5f58c",
   "metadata": {},
   "source": [
    "## 6. Demonstration of a Use Case\n",
    "- **Use Case:** We demonstrate a simple use case by performing a word frequency analysis on the \"Technology\" category data.\n",
    "- This can be useful for understanding common terms in technology-related articles.\n",
    "\n",
    "- ### In a real project, you might use this dataset for tasks like topic modeling, sentiment analysis, or clustering. \n",
    "- ### This is done in a separate notebook provided\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d0df02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words in the Technology category:\n",
      "ai: 10\n",
      "new: 8\n",
      "2025: 8\n",
      "game: 7\n",
      "techcrunch: 6\n",
      "meta: 5\n",
      "tech: 5\n",
      "get: 5\n",
      "title: 4\n",
      "content: 4\n"
     ]
    }
   ],
   "source": [
    "# Load the technology category text file\n",
    "tech_file = os.path.join(cleaned_dir, \"Technology.txt\")\n",
    "with open(tech_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    tech_text = f.read()\n",
    "\n",
    "# Tokenize the technology text\n",
    "tokens = word_tokenize(tech_text)\n",
    "\n",
    "# Calculate word frequencies\n",
    "freq = Counter(tokens)\n",
    "\n",
    "# Display the 10 most common words\n",
    "print(\"Most common words in the Technology category:\")\n",
    "for word, count in freq.most_common(10):\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df3f4e7",
   "metadata": {},
   "source": [
    "## 7. Summary\n",
    "- In this notebook we:\n",
    " - Defined 20 categories with three websites each.\n",
    "  - Implemented a basic web crawler to extract article titles, content, and publication dates.\n",
    " - Cleaned the text data using NLP preprocessing techniques.\n",
    " - Stored the data into separate text files for each category.\n",
    " - Named the dataset as **IITG_MultimodalTextDataset**.\n",
    " - Demonstrated a simple use case (word frequency analysis) on the Technology category.\n",
    "### This framework can be extended further by refining the extraction methods and applying more advanced NLP techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc098863",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
